
Given the MHz scale repetition rate of Table~\ref{lcls2specs} for LCLS-II, a measure-and-sort method that required every XFEL shot be recorded would increase the data load from the 10 TB/day of today to 100 PB/day.
Such a load would require an enormous cost for developing the ultra-high duty-cycle area detectors and the corresponding network and storage infrastructure. 
If instead one could use real-time information about the x-ray pulse shape, then one could veto events where the pulse characteristics were not amenable to the physics being measured.
Furthermore, one could allow for on-board histogram updates in a memory buffer that could collate the real-time sorted results prior to network transfer.
Such a vision fuels our strong motivation to not only provide high repetition rate veto, but also event rate, low-latency sorting triggers.

The on-board analysis of the data is a challenging bottleneck in the angular streaking scheme.
The raw data in angular streaking is the digitized waveform spanning about 500~ns of record length with a sample frequency of ideally about 10GS/s, one waveform for each of the 16 detectors.
Transferring and writing this data would require nearly 160~GBps steady-state continuous feed.
We therefore require that the analysis occur ideally directly in combination with the digitization.
All together though, this analysis would be comparable to analyzing one $256\times256$ 10 bit deep image every microsecond.

Detailed in Ref.~\cite{Nick2018}, we iteratively account for intensity in the polar representation of the angular photo-electron spectrum.
This so called ``PacMan'' routine as well as a similar methods \cite{Thomas2015,Siqi2018,HJWorner2018} and certainly ``Attoclock Ptychography'' \cite{Thomas2018} are computationally expensive and are not likely to allow MHz or even 100 kHz data throughput.
They certainly will not achieve the desired microsecond scale latency for providing sort and veto triggers.
We therefore plan to reserve such computationally expensive methods for the generation of so-called ``ground truth'' x-ray pulse shapes for a sub-set of full fidelity raw data.

We plan to use the ground truth set for training and validating a low-latency inference matrix solution that could be implemented as a series of FPGA-based on-board matrix multiplications.
Only very small data of the retrieved pulse would be transferred to remote data recording nodes along with intermittent high fidelity shots for continually populating the validation and re-training sets.
We will use simulations of angular streaking from expected FEL pulses, together with the detector array simulations used for the design modelling, to iterate on the detector hardware configurations and the analysis pipeline from electronics to inference output.
We will use existing data to build a repository of example waveforms with realistic expectations for the eTOF point-spread functions.  

The inference model will be trained based on a transfer learning paradigm whereby the central hidden layers, e.g. the simulation trained layers, will be optimized to recover x-ray pulse shapes given simulated angular streaking results.
The next step of the model training then adds a thin surface set of layers at the input side that is connected to the raw data of the detector.
We expect that such an architecture will preserve the physics of angular streaking as contained in the simulations, while also incorporating the raw sensor calibrations as well as adjusting to any remaining systematic errors in the retrieval interpretation.
By performing different algorithms on the same data, we can benchmark the pre-analysis that will ultimately be used to enhance resolution in the retrieval while maintaining the very high throughput needed for real-time analysis.

This chain of analysis will be compiled for FPGA deployment.  
By deploying to FPGA, we will only need to transfer the reconstructed x-ray pulse description rather than each of the individual raw waveforms. 
We will leverage our close collaboration with the Stanford CS/EE group of Kunle Olukotun, in particular though a shared Graduate Student Fellow, to deploy the analysis chain to FPGA as a very high throughput ($10^6$ Fps) and low latency ($\sim10~\mu$s) streaming inference engine.

