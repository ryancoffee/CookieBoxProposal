Given the MHz scale repetition rate of Table~\ref{lcls2specs} for LCLS-II, a measure-and-sort method that required every XFEL shot be recorded would increase the data load from the 10 TB/day of today to 100 PB/day.
Such a load would require an enormous cost for developing the ultra-high duty-cycle area detectors and the corresponding network and storage infrastructure. 
If instead one could use real-time information about the x-ray pulse shape, then one could veto events where the pulse characteristics were not amenable to the physics being measured.
Furthermore, one could allow for on-board histogram updates in a memory buffer that could coallate the real-time sorted results prior to network transfer.
Such a vision fuels out strong motivation to not only provide high repetition rate veto, but also event rate, low-latency sorting triggers.

The on-board analysis of the data is a challenging bottleneck in the angular streaking scheme.
The raw data in angular streaking is the digitized waveform spanning about 200--500~ns of record length with a sample frequency of ideally about 10GS/s, one waveform for each of the 16 detectors.
All together this would be comparable to analyzing one $256\times256$ image every microsecond.
Detailed in Ref.~\cite{Nick2018}, we iteratively account for intensity in the polar representation of the angular photo-electron spectrum.
This so called ``PacMan'' routine as well as a similar method of projections \cite{Siqi2018} are computationally expensive and would not likely allow for MHz or even 100 kHz repetition rate.
We therefore plan to reserve these and other retrieval methods \cite{Thomas2015,HJWorner2018} for the generation of so-called ``ground truth'' for a sub-set of full fidelity recorded x-ray pulse shapes.

We plan to use the full fidelity ground truth set for training and validating a low-latency inference matrix solution that could be implemented as a series of FPGA-based on-board matrix multiplications.
Only very small data of the retrieved pulse would be transferred to remote data recording nodes along with intermittent high fidelity shots for continually populating the validation and re-training sets.
We will use simulations of angular streaking from expected FEL pulses, together with the detector array simulations used for the design modelling, to iterate on the detector hardware configurations and the analysis pipeline from electronics to inference output.

\begin{comment}
The inference model will be trained based on a transfer learning paradigm whereby the central hidden layers, e.g. the simulation trained core layers, will be optimized to recover x-ray pulse shapes (ground truth) given simulated angular streaking results.
Data handling is a key challenge for which we plan to use a FPGA-based machine learning inferencing.
The next step of the model training then adds a thin surface set of layers at the input side that is connected to the raw data of the detector.
Also, we will connect via a similarly thin set of layers that connect the preliminary output to the now data-based pulse reconstructions.
We expect that such an architecture will preserve the physics of angular streaking as contained in the simulations, while also incorporating the raw sensor calibrations as well as adjusting to any remaining systematic errors in the retrieval interpretation.
\end{comment}



